{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ebd221bc5d2043a4b94dcbce3cc83af6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c53d2717141d48919756f4b2fb9bac5c","IPY_MODEL_2cd329cde79643a49bce69e2dc0efc5e","IPY_MODEL_777ba29e1415443a8af1e6284af70590"],"layout":"IPY_MODEL_ae3a18e9c7d248d7b75edaffd7681bd1"}},"c53d2717141d48919756f4b2fb9bac5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab2f85ca57db433e8dd5fd6e9d34a0ce","placeholder":"​","style":"IPY_MODEL_33df445807754aec834c5ce3c9ab5dbf","value":"Loading checkpoint shards: 100%"}},"2cd329cde79643a49bce69e2dc0efc5e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8df334aecd1c4109a41a2c31f87b81ac","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92e5ba50fc584d8aa8dd798b8d67336c","value":2}},"777ba29e1415443a8af1e6284af70590":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62d2482b97e44b2eaf367222a3c0cb1c","placeholder":"​","style":"IPY_MODEL_d637ca221c9c455cb8ae1e0e2dab2964","value":" 2/2 [00:29&lt;00:00, 12.42s/it]"}},"ae3a18e9c7d248d7b75edaffd7681bd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab2f85ca57db433e8dd5fd6e9d34a0ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33df445807754aec834c5ce3c9ab5dbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8df334aecd1c4109a41a2c31f87b81ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92e5ba50fc584d8aa8dd798b8d67336c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62d2482b97e44b2eaf367222a3c0cb1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d637ca221c9c455cb8ae1e0e2dab2964":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80e8ca4a98fc4afeb43c9caa5bc57aa2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16021ce0a4c5470a973b2f9d5f48c031","IPY_MODEL_9dd4eabeaa404d51940d52d5d8fbb801","IPY_MODEL_ad4bcb2c8e3447ba8c0655be8c420fec"],"layout":"IPY_MODEL_b7168b665c9d4f76a02319f30d88ffe2"}},"16021ce0a4c5470a973b2f9d5f48c031":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8413424a91e4345a591e10e08a9bd48","placeholder":"​","style":"IPY_MODEL_9baac74adc45448290b3af425c711d69","value":"Loading checkpoint shards: 100%"}},"9dd4eabeaa404d51940d52d5d8fbb801":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e22416dd8a47481e80ade76275989629","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ac62eb27b1349459df5501441b0115c","value":2}},"ad4bcb2c8e3447ba8c0655be8c420fec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1c1deba2fd14a698b04ba5e96de2cd1","placeholder":"​","style":"IPY_MODEL_6d7852e394b54cbd810d87d8edfdd97f","value":" 2/2 [00:35&lt;00:00, 14.86s/it]"}},"b7168b665c9d4f76a02319f30d88ffe2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8413424a91e4345a591e10e08a9bd48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9baac74adc45448290b3af425c711d69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e22416dd8a47481e80ade76275989629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ac62eb27b1349459df5501441b0115c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1c1deba2fd14a698b04ba5e96de2cd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d7852e394b54cbd810d87d8edfdd97f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11137188,"sourceType":"datasetVersion","datasetId":6946631}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is my first time posting code. If there's anything I can improve, feel free to share your feedback!","metadata":{}},{"cell_type":"markdown","source":"Downloading the dependencies","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade datasets\n!pip install --upgrade transformers\n!pip install --upgrade pert\n!pip install --upgrade trl\n!pip install accelerate\n!pip install bitsandbytes\n!pip install tensorboard","metadata":{"id":"cPRakqObCcnx","outputId":"fbc293a5-b8cd-4792-d006-96e1abee4676","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:42:34.715844Z","iopub.execute_input":"2025-03-23T13:42:34.716287Z","iopub.status.idle":"2025-03-23T13:43:15.510923Z","shell.execute_reply.started":"2025-03-23T13:42:34.716241Z","shell.execute_reply":"2025-03-23T13:43:15.509812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Connect to Hugging Face ","metadata":{}},{"cell_type":"code","source":"# from google.colab import userdata\n# from huggingface_hub import login\n\n# # Login into Hugging Face Hub\n# hf_token = userdata.get('HF_TOKEN') # If you are running inside a Google Colab\n# login(hf_token)\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\nfrom huggingface_hub.hf_api import HfFolder\nHfFolder.save_token(\"YOUR_KEY\")","metadata":{"id":"tbFHNsO5kfxr","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:43:18.296019Z","iopub.execute_input":"2025-03-23T13:43:18.296348Z","iopub.status.idle":"2025-03-23T13:43:19.218088Z","shell.execute_reply.started":"2025-03-23T13:43:18.296320Z","shell.execute_reply":"2025-03-23T13:43:19.217425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After getting the aknowledgement of your gemma model licence from hugging face download the model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"google/gemma-2b\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprint(model)","metadata":{"id":"cf4a7KK-vka4","outputId":"85695b51-2c97-4502-a2a7-9c2ff9e6551a","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:43:22.716008Z","iopub.execute_input":"2025-03-23T13:43:22.716292Z","iopub.status.idle":"2025-03-23T13:44:17.687841Z","shell.execute_reply.started":"2025-03-23T13:43:22.716272Z","shell.execute_reply":"2025-03-23T13:44:17.686992Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Test the downloaded model ","metadata":{}},{"cell_type":"code","source":"input_text = \"what is anime?\"\n\ninput_ids = tokenizer(input_text, return_tensors='pt')\noutput = model.generate(**input_ids, max_length=128)\nprint(tokenizer.decode(output[0]))","metadata":{"id":"Ryr4EsHKw8Ki","outputId":"0060809f-0d78-4331-89f6-19316255ade9","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:44:25.014969Z","iopub.execute_input":"2025-03-23T13:44:25.015566Z","iopub.status.idle":"2025-03-23T13:45:21.869146Z","shell.execute_reply.started":"2025-03-23T13:44:25.015537Z","shell.execute_reply":"2025-03-23T13:45:21.868350Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"import your dataset in json file.   \n**NOTE: make sure your dataset is labeled dataset (i.e. that it has both the dependent values and independent values)**","metadata":{}},{"cell_type":"code","source":"import json\n# Load dataset\nfile_path = \"/kaggle/input/promogen-001-dataset/PromoGen_001.json\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n","metadata":{"id":"KYuR2ifdkgt0","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:45:25.014553Z","iopub.execute_input":"2025-03-23T13:45:25.014896Z","iopub.status.idle":"2025-03-23T13:45:25.050198Z","shell.execute_reply.started":"2025-03-23T13:45:25.014871Z","shell.execute_reply":"2025-03-23T13:45:25.049244Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"check if the correct dataset is loaded  ","metadata":{}},{"cell_type":"code","source":"print(data[0])","metadata":{"id":"kn48LZhJzgrK","outputId":"26d5a546-853b-473b-9f00-c4b60ea63cff","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:45:27.457311Z","iopub.execute_input":"2025-03-23T13:45:27.457653Z","iopub.status.idle":"2025-03-23T13:45:27.462378Z","shell.execute_reply.started":"2025-03-23T13:45:27.457625Z","shell.execute_reply":"2025-03-23T13:45:27.461426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(data))","metadata":{"id":"6cwZ45Knzld2","outputId":"3ad957ec-f25d-43fd-8a9e-3c8499da4db7","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:45:31.512549Z","iopub.execute_input":"2025-03-23T13:45:31.512856Z","iopub.status.idle":"2025-03-23T13:45:31.517372Z","shell.execute_reply.started":"2025-03-23T13:45:31.512832Z","shell.execute_reply":"2025-03-23T13:45:31.516468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"convert your dataset into your required prompt that you want the model to learn, in my case it was advertisement generation based on the company name , product name, product description , ad script and CTA. You can make your own prompt.","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n# Convert JSON to Hugging Face dataset using the instruction-based template\ndef format_data(example):\n    return {\n        \"text\": (\n            \"### Instruction: Write a highly engaging advertisement script and a strong call to action for the product below. \\n\"\n            \"The ad script should creatively describe the product's features and benefits, and the call to action should motivate the audience to take action immediately. \\n\\n\"\n            f\"Company Name: {example['Company Name']}\\n\"\n            f\"Product Name: {example['Product Name']}\\n\"\n            f\"Product Description: {example['Product Description']}\\n\\n\"\n            \"Ad Script:\\n\"\n            f\"{example['Ad Script']}\\n\\n\"\n            \"CTA:\\n\"\n            f\"{example['CTA']}\"\n        )\n    }\n\nformatted_data = [format_data(d) for d in data]\ndataset = Dataset.from_list(formatted_data)","metadata":{"id":"uhlaf3G6kgTR","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:45:32.683856Z","iopub.execute_input":"2025-03-23T13:45:32.684199Z","iopub.status.idle":"2025-03-23T13:45:33.263629Z","shell.execute_reply.started":"2025-03-23T13:45:32.684171Z","shell.execute_reply":"2025-03-23T13:45:33.262912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset[0])","metadata":{"id":"pKBFE3Cq28Mw","outputId":"375cfa9c-3c3b-41aa-a94b-1a7cae0fe8b9","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:45:36.873427Z","iopub.execute_input":"2025-03-23T13:45:36.873766Z","iopub.status.idle":"2025-03-23T13:45:36.881447Z","shell.execute_reply.started":"2025-03-23T13:45:36.873740Z","shell.execute_reply":"2025-03-23T13:45:36.880566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Tokenizing the dataset**  \nThis this where we convert the text into tokens.  \ntruncate : this ensure that if the text is longer than the max lenght, it gets cut off.  \npadding : All the tokenize ouput are of same length.  \nmax length : fixed length of the tokenized sequence.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128\n    )\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"id":"0X7QoOFH43up","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:45:44.588764Z","iopub.execute_input":"2025-03-23T13:45:44.589064Z","iopub.status.idle":"2025-03-23T13:45:45.591138Z","shell.execute_reply.started":"2025-03-23T13:45:44.589042Z","shell.execute_reply":"2025-03-23T13:45:45.590182Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Import the dependencies for the model ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"kQK57ltL66Jw","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:45:47.085053Z","iopub.execute_input":"2025-03-23T13:45:47.085498Z","iopub.status.idle":"2025-03-23T13:45:50.205442Z","shell.execute_reply.started":"2025-03-23T13:45:47.085439Z","shell.execute_reply":"2025-03-23T13:45:50.204566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model_name = \"google/gemma-2b\" #base model name\nnew_model = \"gemma-ft\" #new fine-tuned modle name\n\n# Lora Adaptation Parameters\nlora_r = 4 # Low rank dimension (smaller r reduces the memory usages but it may impact the model performance)\nlora_alpha = 16 #scaling factor for LoRA updates\nlora_dropout = 0.1 # applies droput for preventing overfitting\n\n#Bit quantization Setup(4-bit)\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nbnb_4bit_use_double_quant = True\n\n#training hyperparameters\noutput_dir = \"./promogen_001\"\nnum_train_epochs = 4\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 25\n\nmax_seq_length = 128\npacking = False\ndevice_map = \"auto\"","metadata":{"id":"BeMoenr73OQ5","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:45:52.131724Z","iopub.execute_input":"2025-03-23T13:45:52.132052Z","iopub.status.idle":"2025-03-23T13:45:52.137822Z","shell.execute_reply.started":"2025-03-23T13:45:52.132026Z","shell.execute_reply":"2025-03-23T13:45:52.136817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"here we are configuring the model in 4-bit quantization","metadata":{}},{"cell_type":"code","source":"compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n)","metadata":{"id":"XiiUd7nh6Q1O","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:45:54.889071Z","iopub.execute_input":"2025-03-23T13:45:54.889367Z","iopub.status.idle":"2025-03-23T13:45:54.894693Z","shell.execute_reply.started":"2025-03-23T13:45:54.889344Z","shell.execute_reply":"2025-03-23T13:45:54.893899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we load the pre-trained model with 4-bit quantization and tokenization.","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    token = hf_token,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name,\n                                          token=hf_token,\n                                          trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"id":"Qi84bBFX9Txi","outputId":"2127cbf7-d99c-48e6-93a9-6d47fa5ad946","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:45:56.509889Z","iopub.execute_input":"2025-03-23T13:45:56.510186Z","iopub.status.idle":"2025-03-23T13:46:05.931820Z","shell.execute_reply.started":"2025-03-23T13:45:56.510163Z","shell.execute_reply":"2025-03-23T13:46:05.930861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Configures the LoRA settings for the gemma model by using peft","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\"],\n)","metadata":{"id":"7JZ-WhMj-Mpj","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:46:07.825296Z","iopub.execute_input":"2025-03-23T13:46:07.825638Z","iopub.status.idle":"2025-03-23T13:46:07.829512Z","shell.execute_reply.started":"2025-03-23T13:46:07.825611Z","shell.execute_reply":"2025-03-23T13:46:07.828771Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Configures the training argument which controls how the model is going to be fine-tuned.","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\",\n)\ntraining_arguments\n","metadata":{"id":"wPdPyr4CAH9t","outputId":"ff4df22e-6a9d-48fb-b28b-cf124e06d6cd","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:46:09.201508Z","iopub.execute_input":"2025-03-23T13:46:09.201844Z","iopub.status.idle":"2025-03-23T13:46:09.233626Z","shell.execute_reply.started":"2025-03-23T13:46:09.201819Z","shell.execute_reply":"2025-03-23T13:46:09.232756Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we initialize the SFTT(Supervised Fine Tuning Training) using hugging face TRL(transformer Reinforcement Learning)","metadata":{}},{"cell_type":"code","source":"\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_datasets,\n    peft_config=peft_config,\n    args=training_arguments,\n    # max_seq_length=max_seq_length,\n\n)\n","metadata":{"id":"8-CvIZ8eAmzU","outputId":"1b5b6fa1-1e4d-44a7-d385-68e6a14cb6d0","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:46:15.931658Z","iopub.execute_input":"2025-03-23T13:46:15.931975Z","iopub.status.idle":"2025-03-23T13:46:17.770497Z","shell.execute_reply.started":"2025-03-23T13:46:15.931952Z","shell.execute_reply":"2025-03-23T13:46:17.769782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Tokenized Dataset Size Before Training: {len(tokenized_datasets)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:46:19.268094Z","iopub.execute_input":"2025-03-23T13:46:19.268458Z","iopub.status.idle":"2025-03-23T13:46:19.273428Z","shell.execute_reply.started":"2025-03-23T13:46:19.268417Z","shell.execute_reply":"2025-03-23T13:46:19.272398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Trainer Dataset Size: {len(trainer.train_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:46:22.828278Z","iopub.execute_input":"2025-03-23T13:46:22.828624Z","iopub.status.idle":"2025-03-23T13:46:22.833106Z","shell.execute_reply.started":"2025-03-23T13:46:22.828598Z","shell.execute_reply":"2025-03-23T13:46:22.832285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we start the training of the dataset","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"v_wZcC8c5VwX","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:46:24.152109Z","iopub.execute_input":"2025-03-23T13:46:24.152399Z","iopub.status.idle":"2025-03-23T14:22:37.636948Z","shell.execute_reply.started":"2025-03-23T13:46:24.152376Z","shell.execute_reply":"2025-03-23T14:22:37.636208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Then we save the model","metadata":{}},{"cell_type":"code","source":"trainer.save_model(new_model)","metadata":{"id":"YZj5a6sM5dTo","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T14:23:11.016889Z","iopub.execute_input":"2025-03-23T14:23:11.017182Z","iopub.status.idle":"2025-03-23T14:23:12.078981Z","shell.execute_reply.started":"2025-03-23T14:23:11.017160Z","shell.execute_reply":"2025-03-23T14:23:12.078291Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we loads and merge the base model and the fine-tunned model and save it","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\n\n# Load LoRA adapter (fine-tuned weights)\nmodel = PeftModel.from_pretrained(base_model, new_model)\n\n# Merge LoRA weights into the base model\nmodel = model.merge_and_unload()\n\n# Save the merged model for deployment\nmodel.save_pretrained(\"promogen_final_model\")\n\n# Reload and save tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\ntokenizer.save_pretrained(\"promogen_final_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T14:23:15.181905Z","iopub.execute_input":"2025-03-23T14:23:15.182192Z","iopub.status.idle":"2025-03-23T14:23:35.166730Z","shell.execute_reply.started":"2025-03-23T14:23:15.182170Z","shell.execute_reply":"2025-03-23T14:23:35.165836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we test the saved model ","metadata":{}},{"cell_type":"code","source":"\nimport re\ncompany_name = input(\"Enter Company Name: \")\nproduct_name = input(\"Enter Product Name: \")\nproduct_description = input(\"Enter Product Description: \")\n\n# Create Prompt\ninput_text = (\n    \"### Instruction: Write a highly engaging advertisement script and a strong call to action for the product below.\\n\"\n    \"The ad script should creatively describe the product's features and benefits, and the call to action should motivate the audience to take action immediately.\\n\\n\"\n    f\"Company Name: {company_name}\\n\"\n    f\"Product Name: {product_name}\\n\"\n    f\"Product Description: {product_description}\\n\\n\"\n    \"Ad Script:\\n\"\n)\n\n# Tokenize and move to GPU\ninputs = tokenizer(input_text, return_tensors=\"pt\")\ninputs = {key: val.to(\"cuda\") for key, val in inputs.items()} \n\n# Generate output\n# Generate output (this is a tensor)\noutputs = model.generate(**inputs, max_length=256,temperature=0.7,top_p=0.9,do_sample=True)\n\n# Decode the tensor output to get a string\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Regex to extract ONLY the first Ad Script and CTA before \"Company Name\" repeats\npattern = r\"Ad Script:\\s*(.*?)\\s*CTA:\\s*(.*?)(?:\\nCompany Name:|\\Z)\"\nmatch = re.search(pattern, generated_text, re.DOTALL)\n\nif match:\n    ad_script = match.group(1).strip()\n    cta = match.group(2).strip()\n    print(\"\\n Ad Script:\\n\", ad_script)\n    print(\"\\n Call to Action:\\n\", cta)\nelse:\n    print(\"\\n Couldn't find Ad Script and CTA properly.\\n\")\n    print(generated_text)\n","metadata":{"id":"r23oI6z96IPz","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T14:23:45.905047Z","iopub.execute_input":"2025-03-23T14:23:45.905332Z","iopub.status.idle":"2025-03-23T14:24:54.324558Z","shell.execute_reply.started":"2025-03-23T14:23:45.905311Z","shell.execute_reply":"2025-03-23T14:24:54.323799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !zip -r /kaggle/working/promogen_001.zip /kaggle/working/promogen_001\n!zip -r promogen_final_model.zip promogen_final_model/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T14:25:06.881425Z","iopub.execute_input":"2025-03-23T14:25:06.881760Z","iopub.status.idle":"2025-03-23T14:31:51.675566Z","shell.execute_reply.started":"2025-03-23T14:25:06.881735Z","shell.execute_reply":"2025-03-23T14:31:51.674638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'promogen_final_model.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T14:33:02.335377Z","iopub.execute_input":"2025-03-23T14:33:02.335779Z","iopub.status.idle":"2025-03-23T14:33:02.341831Z","shell.execute_reply.started":"2025-03-23T14:33:02.335750Z","shell.execute_reply":"2025-03-23T14:33:02.341016Z"}},"outputs":[],"execution_count":null}]}